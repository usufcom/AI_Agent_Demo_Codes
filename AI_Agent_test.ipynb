{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1de662b",
   "metadata": {},
   "source": [
    "# Agentic AI and RAG Demo\n",
    "\n",
    "This notebook demonstrates two key AI concepts:\n",
    "\n",
    "1. **Agentic AI**: Direct interaction with Large Language Models (LLMs) via API\n",
    "2. **RAG (Retrieval-Augmented Generation)**: Enhanced AI responses using document knowledge bases\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Basic AI Agent\n",
    "\n",
    "This section shows a simple AI agent that uses OpenRouter API to interact with free LLM models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074c0e29",
   "metadata": {},
   "source": [
    "### How the AI Agent Works:\n",
    "\n",
    "1. **API Setup**: Uses OpenRouter API to access multiple free LLM models\n",
    "2. **Model Selection**: Choose from various free models (Grok, GPT-OSS, DeepSeek, etc.)\n",
    "3. **Message Format**: Structures conversation with system and user messages\n",
    "4. **API Call**: Sends request to OpenRouter with model, messages, and parameters\n",
    "5. **Response Handling**: Parses and displays the AI's response\n",
    "\n",
    "**Key Parameters:**\n",
    "- `temperature`: Controls randomness (0.2 = more focused, 1.0 = more creative)\n",
    "- `max_tokens`: Maximum length of the response\n",
    "- `model`: The specific LLM to use (free models available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f915cfee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRemoteDisconnected\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\http\\client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\http\\client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\http\\client.py:294\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[32m    292\u001b[39m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[33m\"\u001b[39m\u001b[33mRemote end closed connection without\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    295\u001b[39m                              \u001b[33m\"\u001b[39m\u001b[33m response\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mRemoteDisconnected\u001b[39m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mProtocolError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\site-packages\\urllib3\\util\\retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\site-packages\\urllib3\\util\\util.py:38\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value.__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\http\\client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\http\\client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\http\\client.py:294\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[32m    292\u001b[39m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[33m\"\u001b[39m\u001b[33mRemote end closed connection without\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    295\u001b[39m                              \u001b[33m\"\u001b[39m\u001b[33m response\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mProtocolError\u001b[39m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     27\u001b[39m messages = [\n\u001b[32m     28\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant.\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     29\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mTell me something about AI in Africa.\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     30\u001b[39m ]\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://openrouter.ai/api/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAuthorization\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBearer \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mOPENROUTER_API_KEY\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mContent-Type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4096\u001b[39;49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Parse and print\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.ok:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\site-packages\\requests\\api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\anaconda3\\envs\\AI_Agent_env\\Lib\\site-packages\\requests\\adapters.py:659\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    644\u001b[39m     resp = conn.urlopen(\n\u001b[32m    645\u001b[39m         method=request.method,\n\u001b[32m    646\u001b[39m         url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    655\u001b[39m         chunked=chunked,\n\u001b[32m    656\u001b[39m     )\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    662\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, ConnectTimeoutError):\n\u001b[32m    663\u001b[39m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[31mConnectionError\u001b[39m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import requests\n",
    "import textwrap\n",
    "\n",
    "\n",
    "# Your OpenRouter API Key\n",
    "load_dotenv()\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "\n",
    "# Choose your model \n",
    "# Replace with any available model\n",
    "MODEL = \"x-ai/grok-4.1-fast:free\"\n",
    "\n",
    "'''\n",
    "Free Models to use:\n",
    "1. \"openai/gpt-oss-20b:free\" \n",
    "2. \"tngtech/deepseek-r1t2-chimera:free\"  \n",
    "3. \"google/gemma-3-27b-it:free\"\n",
    "4. \"qwen/qwen3-coder:free\"\n",
    "\n",
    "'''\n",
    "\n",
    "# Compose the conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me something about AI in Africa.\"}\n",
    "]\n",
    "\n",
    "# Send the request\n",
    "response = requests.post(\n",
    "    \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    },\n",
    "    json={\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 4096\n",
    "    }\n",
    ")\n",
    "\n",
    "# Parse and print\n",
    "if response.ok:\n",
    "    result = response.json()\n",
    "    result_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"AI Response:\\n {textwrap.fill(result_text, width=100)}\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e1276",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Function Calling (Tool Use) in Agentic AI\n",
    "\n",
    "Function calling allows AI agents to use external tools and functions, making them more powerful and capable of performing actions beyond just text generation.\n",
    "\n",
    "### How Function Calling Works:\n",
    "\n",
    "1. **Define Functions**: Create functions that the AI can call (tools)\n",
    "2. **Describe Functions**: Provide function schemas (name, description, parameters)\n",
    "3. **AI Decides**: The LLM decides when and which function to call\n",
    "4. **Execute Function**: Run the function with provided parameters\n",
    "5. **Return Results**: Send function results back to the AI for final response\n",
    "\n",
    "### Example Use Cases:\n",
    "- Mathematical calculations\n",
    "- Data lookups (databases, APIs)\n",
    "- File operations\n",
    "- External service integrations\n",
    "- Custom business logic\n",
    "\n",
    "### Note on Free Models:\n",
    "Some free models may have limited or no function calling support. If you encounter errors, try:\n",
    "- Using a different free model (e.g., \"openai/gpt-oss-20b:free\")\n",
    "- The code includes proper error handling to show what went wrong\n",
    "- Function calling format follows OpenRouter API specification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b14e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Calling Demo: AI Agent with Tools\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import requests\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "load_dotenv()\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# Use a model that supports function calling\n",
    "# Note: Some free models may have limited or no function calling support\n",
    "# If you get errors, try switching to a different model\n",
    "MODEL = \"x-ai/grok-4.1-fast:free\"  \n",
    "# Alternative free models to try:\n",
    "# MODEL = \"openai/gpt-oss-20b:free\"\n",
    "# MODEL = \"google/gemma-3-27b-it:free\"\n",
    "\n",
    "# ========== Define Tools (Functions) ==========\n",
    "\n",
    "def calculate(expression):\n",
    "    \"\"\"\n",
    "    Performs mathematical calculations safely.\n",
    "    \n",
    "    Args:\n",
    "        expression: A mathematical expression as a string (e.g., \"2 + 2\", \"sqrt(16)\")\n",
    "    \n",
    "    Returns:\n",
    "        The result of the calculation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Safe evaluation with limited functions\n",
    "        allowed_names = {\n",
    "            k: v for k, v in math.__dict__.items() if not k.startswith(\"__\")\n",
    "        }\n",
    "        allowed_names.update({\"abs\": abs, \"round\": round, \"min\": min, \"max\": max})\n",
    "        result = eval(expression, {\"__builtins__\": {}}, allowed_names)\n",
    "        return f\"Result: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def get_current_time():\n",
    "    \"\"\"\n",
    "    Gets the current date and time.\n",
    "    \n",
    "    Returns:\n",
    "        Current date and time as a string\n",
    "    \"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def text_uppercase(text):\n",
    "    \"\"\"\n",
    "    Converts text to uppercase.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to convert\n",
    "    \n",
    "    Returns:\n",
    "        Uppercase version of the text\n",
    "    \"\"\"\n",
    "    return text.upper()\n",
    "\n",
    "def text_word_count(text):\n",
    "    \"\"\"\n",
    "    Counts the number of words in a text.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Number of words in the text\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return f\"Word count: {len(words)}\"\n",
    "\n",
    "# Map function names to actual functions\n",
    "available_functions = {\n",
    "    \"calculate\": calculate,\n",
    "    \"get_current_time\": get_current_time,\n",
    "    \"text_uppercase\": text_uppercase,\n",
    "    \"text_word_count\": text_word_count,\n",
    "}\n",
    "\n",
    "# ========== Define Function Schemas ==========\n",
    "# These describe the functions to the AI model\n",
    "# OpenRouter API requires tools to be wrapped with \"type\": \"function\"\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate\",\n",
    "            \"description\": \"Performs mathematical calculations. Supports basic operations (+, -, *, /) and math functions like sqrt, sin, cos, etc.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The mathematical expression to evaluate (e.g., '2 + 2', 'sqrt(16)', 'sin(3.14/2)')\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_time\",\n",
    "            \"description\": \"Gets the current date and time\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"text_uppercase\",\n",
    "            \"description\": \"Converts text to uppercase letters\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The text to convert to uppercase\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"text\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"text_word_count\",\n",
    "            \"description\": \"Counts the number of words in a given text\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The text to count words in\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"text\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# ========== Function Calling Handler ==========\n",
    "\n",
    "def chat_with_functions(user_message, conversation_history=[]):\n",
    "    \"\"\"\n",
    "    Handles conversation with function calling support.\n",
    "    \"\"\"\n",
    "    messages = conversation_history.copy() if conversation_history else []\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # First API call: Let AI decide if it needs to call a function\n",
    "    response = requests.post(\n",
    "        \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        json={\n",
    "            \"model\": MODEL,\n",
    "            \"messages\": messages,\n",
    "            \"tools\": tools,  # Provide available functions\n",
    "            \"tool_choice\": \"auto\",  # Let AI decide when to use tools\n",
    "            \"temperature\": 0.2,\n",
    "            \"max_tokens\": 4096\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if not response.ok:\n",
    "        error_text = response.text\n",
    "        # Provide helpful error message\n",
    "        if \"function\" in error_text.lower() or \"tool\" in error_text.lower():\n",
    "            error_msg = f\"Error: {response.status_code}\\n\"\n",
    "            error_msg += f\"The model '{MODEL}' may not support function calling.\\n\"\n",
    "            error_msg += \"Try switching to a different model (e.g., 'openai/gpt-oss-20b:free')\\n\"\n",
    "            error_msg += f\"Full error: {error_text}\"\n",
    "            return error_msg, messages\n",
    "        return f\"Error: {response.status_code} - {error_text}\", messages\n",
    "    \n",
    "    result = response.json()\n",
    "    assistant_message = result[\"choices\"][0][\"message\"]\n",
    "    messages.append(assistant_message)\n",
    "    \n",
    "    # Check if AI wants to call a function\n",
    "    if \"tool_calls\" in assistant_message:\n",
    "        print(\"üîß AI wants to use a function!\")\n",
    "        \n",
    "        # Execute each function call\n",
    "        for tool_call in assistant_message[\"tool_calls\"]:\n",
    "            function_name = tool_call[\"function\"][\"name\"]\n",
    "            function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "            \n",
    "            print(f\"  ‚Üí Calling: {function_name}({function_args})\")\n",
    "            \n",
    "            # Execute the function\n",
    "            if function_name in available_functions:\n",
    "                function_result = available_functions[function_name](**function_args)\n",
    "                print(f\"  ‚úì Result: {function_result}\")\n",
    "                \n",
    "                # Add function result to conversation\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call[\"id\"],\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": str(function_result)\n",
    "                })\n",
    "            else:\n",
    "                print(f\"  ‚úó Function {function_name} not found!\")\n",
    "        \n",
    "        # Second API call: Get final response with function results\n",
    "        print(\"\\nü§ñ Getting final response with function results...\")\n",
    "        response = requests.post(\n",
    "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"messages\": messages,\n",
    "                \"tools\": tools,\n",
    "                \"temperature\": 0.2,\n",
    "                \"max_tokens\": 4096\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.ok:\n",
    "            result = response.json()\n",
    "            final_message = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "            messages.append({\"role\": \"assistant\", \"content\": final_message})\n",
    "            return final_message, messages\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\", messages\n",
    "    else:\n",
    "        # No function calls needed, return direct response\n",
    "        return assistant_message[\"content\"], messages\n",
    "\n",
    "# ========== Example Usage ==========\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Function Calling Demo\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Example 1: Mathematical calculation\n",
    "print(\"\\nüìù Example 1: Mathematical Calculation\")\n",
    "print(\"-\" * 80)\n",
    "query1 = \"What is the square root of 144 plus 25?\"\n",
    "answer1, _ = chat_with_functions(query1)\n",
    "print(f\"\\nüí¨ User: {query1}\")\n",
    "print(f\"\\nü§ñ AI Response:\\n{textwrap.fill(answer1, width=80)}\")\n",
    "\n",
    "# Example 2: Get current time\n",
    "print(\"\\n\\nüìù Example 2: Get Current Time\")\n",
    "print(\"-\" * 80)\n",
    "query2 = \"What time is it now?\"\n",
    "answer2, _ = chat_with_functions(query2)\n",
    "print(f\"\\nüí¨ User: {query2}\")\n",
    "print(f\"\\nü§ñ AI Response:\\n{textwrap.fill(answer2, width=80)}\")\n",
    "\n",
    "# Example 3: Text processing\n",
    "print(\"\\n\\nüìù Example 3: Text Processing\")\n",
    "print(\"-\" * 80)\n",
    "query3 = \"Convert 'Hello World' to uppercase and count the words\"\n",
    "answer3, _ = chat_with_functions(query3)\n",
    "print(f\"\\nüí¨ User: {query3}\")\n",
    "print(f\"\\nü§ñ AI Response:\\n{textwrap.fill(answer3, width=80)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903daa7b",
   "metadata": {},
   "source": [
    "### Try Your Own Function Calls!\n",
    "\n",
    "Modify the query below to test different function calls. The AI will automatically decide which functions to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed72116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Function Calling - Try your own queries!\n",
    "your_query = \"Calculate 15 * 8 and tell me what time it is\"  # Modify this query\n",
    "\n",
    "answer, conversation = chat_with_functions(your_query)\n",
    "print(f\"\\nüí¨ User: {your_query}\")\n",
    "print(f\"\\nü§ñ AI Response:\\n{textwrap.fill(answer, width=80)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2738dc53",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: RAG (Retrieval-Augmented Generation) Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad5c363",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation) Implementation\n",
    "\n",
    "This section implements RAG to query the ETIIAC_2025_forRAG.pdf document using free models.\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that enhances AI responses by:\n",
    "1. **Retrieving** relevant information from a knowledge base (your documents)\n",
    "2. **Augmenting** the user's query with this retrieved context\n",
    "3. **Generating** more accurate and context-aware answers\n",
    "\n",
    "### RAG Workflow:\n",
    "```\n",
    "User Question ‚Üí Search Vector Store ‚Üí Retrieve Relevant Chunks ‚Üí \n",
    "Combine with Question ‚Üí Send to LLM ‚Üí Get Context-Aware Answer\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "- **Vector Store**: Stores document chunks as embeddings (numerical representations)\n",
    "- **Embeddings**: Convert text into vectors that capture semantic meaning\n",
    "- **Similarity Search**: Find the most relevant document chunks for a query\n",
    "- **LLM Integration**: Use retrieved context to generate accurate answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb935b",
   "metadata": {},
   "source": [
    "## Step 1: Create Vector Store from Documents\n",
    "\n",
    "This step processes your PDF documents and creates a searchable vector store.\n",
    "\n",
    "### What happens here:\n",
    "1. **Load Documents**: Reads PDF files from the `docs` folder\n",
    "2. **Chunk Text**: Splits documents into smaller, manageable pieces (500 tokens with 50 token overlap)\n",
    "3. **Create Embeddings**: Converts each chunk into a vector using OpenAI's embedding model\n",
    "4. **Save Vector Store**: Stores embeddings, text chunks, and metadata in a JSON file\n",
    "\n",
    "### Why chunking?\n",
    "- LLMs have token limits, so we break documents into smaller pieces\n",
    "- Overlapping chunks ensure context isn't lost at boundaries\n",
    "- Smaller chunks allow for more precise retrieval\n",
    "\n",
    "### Why embeddings?\n",
    "- Embeddings convert text into numerical vectors\n",
    "- Similar text has similar vectors (close in vector space)\n",
    "- Enables semantic search (finding meaning, not just keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Import the needed libraries ----------------\n",
    "from VectorStore_v2 import VectorStore\n",
    "import os\n",
    "\n",
    "# Your OpenAI API Key\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ---------------- Example usage ----------------\n",
    "kb_folder = r\"docs\"\n",
    "\n",
    "vectorstore_path = \"vectorstore\"\n",
    "vectorstore_name = \"vector_store.json\"\n",
    "\n",
    "# ‚úÖ Create folder only\n",
    "os.makedirs(vectorstore_path, exist_ok=True)\n",
    "\n",
    "# ‚úÖ Join folder + file name\n",
    "vector_store_path = os.path.join(vectorstore_path, vectorstore_name)\n",
    "\n",
    "# Initialize Vector Store\n",
    "store = VectorStore(api_key=os.getenv(\"OPENAI_API_KEY\"), chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Extract text, create and save Vector Store\n",
    "store.exract_save_vector_store(store, kb_folder, vector_store_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570f4948",
   "metadata": {},
   "source": [
    "## Step 2: Query the Knowledge Base with RAG\n",
    "\n",
    "This step demonstrates how to ask questions and get answers based on your documents.\n",
    "\n",
    "### What happens here:\n",
    "1. **Load Vector Store**: Loads the previously created vector store from JSON\n",
    "2. **Embed Query**: Converts your question into an embedding vector\n",
    "3. **Similarity Search**: Finds the top-k most relevant document chunks using cosine similarity\n",
    "4. **Retrieve Context**: Gets the actual text from the most similar chunks\n",
    "5. **Generate Answer**: Sends query + context to the LLM for a context-aware response\n",
    "\n",
    "### How similarity search works:\n",
    "- **Cosine Similarity**: Measures the angle between query and document vectors\n",
    "- **Top-k Retrieval**: Returns the k most similar chunks (here, k=3)\n",
    "- **Context Assembly**: Combines retrieved chunks to provide comprehensive context\n",
    "\n",
    "### Benefits:\n",
    "- ‚úÖ Answers are grounded in your actual documents\n",
    "- ‚úÖ Can cite specific sources (chunks used)\n",
    "- ‚úÖ Reduces hallucinations (AI making up information)\n",
    "- ‚úÖ Works with documents larger than LLM context windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee23d8d",
   "metadata": {},
   "source": [
    "## Try Your Own Questions!\n",
    "\n",
    "Modify the query below to ask questions about the ETIIAC_2025 document. The RAG system will:\n",
    "- Find the most relevant sections from the document\n",
    "- Use that context to generate an accurate answer\n",
    "- Show you which chunks were used (for transparency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b70baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Import the needed libraries ----------------\n",
    "import os\n",
    "from querykb_v2 import RAG\n",
    "import textwrap\n",
    "\n",
    "# ---------------- Example usage ----------------\n",
    "vectorstore_path = \"vectorstore\"\n",
    "vectorstore_name = \"vector_store.json\"\n",
    "\n",
    "vectorstore_full_path = os.path.join(vectorstore_path, vectorstore_name)\n",
    "\n",
    "rag = RAG(vector_store_path=vectorstore_full_path)\n",
    "\n",
    "\n",
    "system_prompt = \"You are a helpful assistant\"\n",
    "user_msg = \"What is this document about?\"\n",
    "\n",
    "answer, used_context = rag.askAI(user_msg, system_prompt, k=3)\n",
    "\n",
    "print(\"\\nContext used:\\n\", used_context)\n",
    "print(\"\\n\")\n",
    "print(f\"Reply Text:\\n {textwrap.fill(answer, width=80)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7102184a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We've Demonstrated:\n",
    "\n",
    "1. **Agentic AI**: Direct API interaction with LLMs using OpenRouter\n",
    "   - Simple, stateless conversation\n",
    "   - Access to multiple free models\n",
    "   - No document context (general knowledge only)\n",
    "\n",
    "2. **RAG System**: Enhanced AI with document knowledge\n",
    "   - Document processing and chunking\n",
    "   - Vector embeddings for semantic search\n",
    "   - Context-aware question answering\n",
    "   - Grounded in your specific documents\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Feature | Basic AI Agent | RAG System |\n",
    "|---------|---------------|------------|\n",
    "| Knowledge Source | Pre-trained model | Your documents |\n",
    "| Accuracy | General knowledge | Document-specific |\n",
    "| Hallucinations | Possible | Reduced |\n",
    "| Context Window | Limited | Can handle large docs |\n",
    "| Use Case | General Q&A | Domain-specific Q&A |\n",
    "\n",
    "### Next Steps:\n",
    "- Try different questions with the RAG system\n",
    "- Experiment with different chunk sizes and overlap\n",
    "- Adjust the `k` parameter (number of chunks retrieved)\n",
    "- Try different free LLM models for comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b010402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
