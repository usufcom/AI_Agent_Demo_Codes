{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1de662b",
   "metadata": {},
   "source": [
    "# Agentic AI and RAG Demo\n",
    "\n",
    "This notebook demonstrates two key AI concepts:\n",
    "\n",
    "1. **Agentic AI**: Direct interaction with Large Language Models (LLMs) via API\n",
    "2. **RAG (Retrieval-Augmented Generation)**: Enhanced AI responses using document knowledge bases\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Basic AI Agent\n",
    "\n",
    "This section shows a simple AI agent that uses OpenRouter API to interact with free LLM models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074c0e29",
   "metadata": {},
   "source": [
    "### How the AI Agent Works:\n",
    "\n",
    "1. **API Setup**: Uses OpenRouter API to access multiple free LLM models\n",
    "2. **Model Selection**: Choose from various free models (Grok, GPT-OSS, DeepSeek, etc.)\n",
    "3. **Message Format**: Structures conversation with system and user messages\n",
    "4. **API Call**: Sends request to OpenRouter with model, messages, and parameters\n",
    "5. **Response Handling**: Parses and displays the AI's response\n",
    "\n",
    "**Key Parameters:**\n",
    "- `temperature`: Controls randomness (0.2 = more focused, 1.0 = more creative)\n",
    "- `max_tokens`: Maximum length of the response\n",
    "- `model`: The specific LLM to use (free models available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import requests\n",
    "import textwrap\n",
    "\n",
    "\n",
    "# Your OpenRouter API Key\n",
    "load_dotenv()\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "\n",
    "# Choose your model \n",
    "# Replace with any available model\n",
    "MODEL = \"x-ai/grok-4.1-fast:free\"\n",
    "\n",
    "'''\n",
    "Free Models to use:\n",
    "1. \"openai/gpt-oss-20b:free\" \n",
    "2. \"tngtech/deepseek-r1t2-chimera:free\"  \n",
    "3. \"google/gemma-3-27b-it:free\"\n",
    "4. \"qwen/qwen3-coder:free\"\n",
    "\n",
    "'''\n",
    "\n",
    "# Compose the conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me something about AI in Africa.\"}\n",
    "]\n",
    "\n",
    "# Send the request\n",
    "response = requests.post(\n",
    "    \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    },\n",
    "    json={\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 4096\n",
    "    }\n",
    ")\n",
    "\n",
    "# Parse and print\n",
    "if response.ok:\n",
    "    result = response.json()\n",
    "    result_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"AI Response:\\n {textwrap.fill(result_text, width=100)}\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e1276",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Function Calling (Tool Use) in Agentic AI\n",
    "\n",
    "Function calling allows AI agents to use external tools and functions, making them more powerful and capable of performing actions beyond just text generation.\n",
    "\n",
    "### How Function Calling Works:\n",
    "\n",
    "1. **Define Functions**: Create functions that the AI can call (tools)\n",
    "2. **Describe Functions**: Provide function schemas (name, description, parameters)\n",
    "3. **AI Decides**: The LLM decides when and which function to call\n",
    "4. **Execute Function**: Run the function with provided parameters\n",
    "5. **Return Results**: Send function results back to the AI for final response\n",
    "\n",
    "### Example Use Cases:\n",
    "- Mathematical calculations\n",
    "- Data lookups (databases, APIs)\n",
    "- File operations\n",
    "- External service integrations\n",
    "- Custom business logic\n",
    "\n",
    "### Note on Free Models:\n",
    "Some free models may have limited or no function calling support. If you encounter errors, try:\n",
    "- Using a different free model (e.g., \"openai/gpt-oss-20b:free\")\n",
    "- The code includes proper error handling to show what went wrong\n",
    "- Function calling format follows OpenRouter API specification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b14e005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Function Calling Demo\n",
      "================================================================================\n",
      "\n",
      "üìù Example 1: Mathematical Calculation\n",
      "--------------------------------------------------------------------------------\n",
      "üîß AI wants to use a function!\n",
      "  ‚Üí Calling: calculate({'expression': 'sqrt(144) + 25'})\n",
      "  ‚úì Result: Result: 37.0\n",
      "\n",
      "ü§ñ Getting final response with function results...\n",
      "\n",
      "üí¨ User: What is the square root of 144 plus 25?\n",
      "\n",
      "ü§ñ AI Response:\n",
      "**37**  ‚àö144 = 12, and 12 + 25 = 37.\n",
      "\n",
      "\n",
      "üìù Example 2: Get Current Time\n",
      "--------------------------------------------------------------------------------\n",
      "üîß AI wants to use a function!\n",
      "  ‚Üí Calling: get_current_time({})\n",
      "  ‚úì Result: 2025-11-28 17:04:30\n",
      "\n",
      "ü§ñ Getting final response with function results...\n",
      "\n",
      "üí¨ User: What time is it now?\n",
      "\n",
      "ü§ñ AI Response:\n",
      "It's currently 2025-11-28 17:04:30 (UTC).\n",
      "\n",
      "\n",
      "üìù Example 3: Text Processing\n",
      "--------------------------------------------------------------------------------\n",
      "üîß AI wants to use a function!\n",
      "  ‚Üí Calling: text_uppercase({'text': 'Hello World'})\n",
      "  ‚úì Result: HELLO WORLD\n",
      "  ‚Üí Calling: text_word_count({'text': 'Hello World'})\n",
      "  ‚úì Result: Word count: 2\n",
      "\n",
      "ü§ñ Getting final response with function results...\n",
      "\n",
      "üí¨ User: Convert 'Hello World' to uppercase and count the words\n",
      "\n",
      "ü§ñ AI Response:\n",
      "**Uppercase:** HELLO WORLD   **Word count:** 2\n"
     ]
    }
   ],
   "source": [
    "# Function Calling Demo: AI Agent with Tools\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import requests\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "load_dotenv()\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# Use a model that supports function calling\n",
    "# Note: Some free models may have limited or no function calling support\n",
    "# If you get errors, try switching to a different model\n",
    "MODEL = \"x-ai/grok-4.1-fast:free\"  \n",
    "# Alternative free models to try:\n",
    "# MODEL = \"openai/gpt-oss-20b:free\"\n",
    "# MODEL = \"google/gemma-3-27b-it:free\"\n",
    "\n",
    "# ========== Define Tools (Functions) ==========\n",
    "\n",
    "def calculate(expression):\n",
    "    \"\"\"\n",
    "    Performs mathematical calculations safely.\n",
    "    \n",
    "    Args:\n",
    "        expression: A mathematical expression as a string (e.g., \"2 + 2\", \"sqrt(16)\")\n",
    "    \n",
    "    Returns:\n",
    "        The result of the calculation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Safe evaluation with limited functions\n",
    "        allowed_names = {\n",
    "            k: v for k, v in math.__dict__.items() if not k.startswith(\"__\")\n",
    "        }\n",
    "        allowed_names.update({\"abs\": abs, \"round\": round, \"min\": min, \"max\": max})\n",
    "        result = eval(expression, {\"__builtins__\": {}}, allowed_names)\n",
    "        return f\"Result: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def get_current_time():\n",
    "    \"\"\"\n",
    "    Gets the current date and time.\n",
    "    \n",
    "    Returns:\n",
    "        Current date and time as a string\n",
    "    \"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def text_uppercase(text):\n",
    "    \"\"\"\n",
    "    Converts text to uppercase.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to convert\n",
    "    \n",
    "    Returns:\n",
    "        Uppercase version of the text\n",
    "    \"\"\"\n",
    "    return text.upper()\n",
    "\n",
    "def text_word_count(text):\n",
    "    \"\"\"\n",
    "    Counts the number of words in a text.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Number of words in the text\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return f\"Word count: {len(words)}\"\n",
    "\n",
    "# Map function names to actual functions\n",
    "available_functions = {\n",
    "    \"calculate\": calculate,\n",
    "    \"get_current_time\": get_current_time,\n",
    "    \"text_uppercase\": text_uppercase,\n",
    "    \"text_word_count\": text_word_count,\n",
    "}\n",
    "\n",
    "# ========== Define Function Schemas ==========\n",
    "# These describe the functions to the AI model\n",
    "# OpenRouter API requires tools to be wrapped with \"type\": \"function\"\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate\",\n",
    "            \"description\": \"Performs mathematical calculations. Supports basic operations (+, -, *, /) and math functions like sqrt, sin, cos, etc.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The mathematical expression to evaluate (e.g., '2 + 2', 'sqrt(16)', 'sin(3.14/2)')\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_time\",\n",
    "            \"description\": \"Gets the current date and time\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"text_uppercase\",\n",
    "            \"description\": \"Converts text to uppercase letters\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The text to convert to uppercase\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"text\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"text_word_count\",\n",
    "            \"description\": \"Counts the number of words in a given text\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The text to count words in\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"text\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# ========== Function Calling Handler ==========\n",
    "\n",
    "def chat_with_functions(user_message, conversation_history=[]):\n",
    "    \"\"\"\n",
    "    Handles conversation with function calling support.\n",
    "    \"\"\"\n",
    "    messages = conversation_history.copy() if conversation_history else []\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # First API call: Let AI decide if it needs to call a function\n",
    "    response = requests.post(\n",
    "        \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        json={\n",
    "            \"model\": MODEL,\n",
    "            \"messages\": messages,\n",
    "            \"tools\": tools,  # Provide available functions\n",
    "            \"tool_choice\": \"auto\",  # Let AI decide when to use tools\n",
    "            \"temperature\": 0.2,\n",
    "            \"max_tokens\": 4096\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if not response.ok:\n",
    "        error_text = response.text\n",
    "        # Provide helpful error message\n",
    "        if \"function\" in error_text.lower() or \"tool\" in error_text.lower():\n",
    "            error_msg = f\"Error: {response.status_code}\\n\"\n",
    "            error_msg += f\"The model '{MODEL}' may not support function calling.\\n\"\n",
    "            error_msg += \"Try switching to a different model (e.g., 'openai/gpt-oss-20b:free')\\n\"\n",
    "            error_msg += f\"Full error: {error_text}\"\n",
    "            return error_msg, messages\n",
    "        return f\"Error: {response.status_code} - {error_text}\", messages\n",
    "    \n",
    "    result = response.json()\n",
    "    assistant_message = result[\"choices\"][0][\"message\"]\n",
    "    messages.append(assistant_message)\n",
    "    \n",
    "    # Check if AI wants to call a function\n",
    "    if \"tool_calls\" in assistant_message:\n",
    "        print(\"üîß AI wants to use a function!\")\n",
    "        \n",
    "        # Execute each function call\n",
    "        for tool_call in assistant_message[\"tool_calls\"]:\n",
    "            function_name = tool_call[\"function\"][\"name\"]\n",
    "            function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "            \n",
    "            print(f\"  ‚Üí Calling: {function_name}({function_args})\")\n",
    "            \n",
    "            # Execute the function\n",
    "            if function_name in available_functions:\n",
    "                function_result = available_functions[function_name](**function_args)\n",
    "                print(f\"  ‚úì Result: {function_result}\")\n",
    "                \n",
    "                # Add function result to conversation\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call[\"id\"],\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": str(function_result)\n",
    "                })\n",
    "            else:\n",
    "                print(f\"  ‚úó Function {function_name} not found!\")\n",
    "        \n",
    "        # Second API call: Get final response with function results\n",
    "        print(\"\\nü§ñ Getting final response with function results...\")\n",
    "        response = requests.post(\n",
    "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"messages\": messages,\n",
    "                \"tools\": tools,\n",
    "                \"temperature\": 0.2,\n",
    "                \"max_tokens\": 4096\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.ok:\n",
    "            result = response.json()\n",
    "            final_message = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "            messages.append({\"role\": \"assistant\", \"content\": final_message})\n",
    "            return final_message, messages\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\", messages\n",
    "    else:\n",
    "        # No function calls needed, return direct response\n",
    "        return assistant_message[\"content\"], messages\n",
    "\n",
    "# ========== Example Usage ==========\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Function Calling Demo\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Example 1: Mathematical calculation\n",
    "print(\"\\nüìù Example 1: Mathematical Calculation\")\n",
    "print(\"-\" * 80)\n",
    "query1 = \"What is the square root of 144 plus 25?\"\n",
    "answer1, _ = chat_with_functions(query1)\n",
    "print(f\"\\nüí¨ User: {query1}\")\n",
    "print(f\"\\nü§ñ AI Response:\\n{textwrap.fill(answer1, width=80)}\")\n",
    "\n",
    "# Example 2: Get current time\n",
    "print(\"\\n\\nüìù Example 2: Get Current Time\")\n",
    "print(\"-\" * 80)\n",
    "query2 = \"What time is it now?\"\n",
    "answer2, _ = chat_with_functions(query2)\n",
    "print(f\"\\nüí¨ User: {query2}\")\n",
    "print(f\"\\nü§ñ AI Response:\\n{textwrap.fill(answer2, width=80)}\")\n",
    "\n",
    "# Example 3: Text processing\n",
    "print(\"\\n\\nüìù Example 3: Text Processing\")\n",
    "print(\"-\" * 80)\n",
    "query3 = \"Convert 'Hello World' to uppercase and count the words\"\n",
    "answer3, _ = chat_with_functions(query3)\n",
    "print(f\"\\nüí¨ User: {query3}\")\n",
    "print(f\"\\nü§ñ AI Response:\\n{textwrap.fill(answer3, width=80)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903daa7b",
   "metadata": {},
   "source": [
    "### Try Your Own Function Calls!\n",
    "\n",
    "Modify the query below to test different function calls. The AI will automatically decide which functions to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed72116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Function Calling - Try your own queries!\n",
    "your_query = \"Calculate 15 * 8 and tell me what time it is\"  # Modify this query\n",
    "\n",
    "answer, conversation = chat_with_functions(your_query)\n",
    "print(f\"\\nüí¨ User: {your_query}\")\n",
    "print(f\"\\nü§ñ AI Response:\\n{textwrap.fill(answer, width=80)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2738dc53",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: RAG (Retrieval-Augmented Generation) Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad5c363",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation) Implementation\n",
    "\n",
    "This section implements RAG to query the ETIIAC_2025_forRAG.pdf document using free models.\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that enhances AI responses by:\n",
    "1. **Retrieving** relevant information from a knowledge base (your documents)\n",
    "2. **Augmenting** the user's query with this retrieved context\n",
    "3. **Generating** more accurate and context-aware answers\n",
    "\n",
    "### RAG Workflow:\n",
    "```\n",
    "User Question ‚Üí Search Vector Store ‚Üí Retrieve Relevant Chunks ‚Üí \n",
    "Combine with Question ‚Üí Send to LLM ‚Üí Get Context-Aware Answer\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "- **Vector Store**: Stores document chunks as embeddings (numerical representations)\n",
    "- **Embeddings**: Convert text into vectors that capture semantic meaning\n",
    "- **Similarity Search**: Find the most relevant document chunks for a query\n",
    "- **LLM Integration**: Use retrieved context to generate accurate answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb935b",
   "metadata": {},
   "source": [
    "## Step 1: Create Vector Store from Documents\n",
    "\n",
    "This step processes your PDF documents and creates a searchable vector store.\n",
    "\n",
    "### What happens here:\n",
    "1. **Load Documents**: Reads PDF files from the `docs` folder\n",
    "2. **Chunk Text**: Splits documents into smaller, manageable pieces (500 tokens with 50 token overlap)\n",
    "3. **Create Embeddings**: Converts each chunk into a vector using OpenAI's embedding model\n",
    "4. **Save Vector Store**: Stores embeddings, text chunks, and metadata in a JSON file\n",
    "\n",
    "### Why chunking?\n",
    "- LLMs have token limits, so we break documents into smaller pieces\n",
    "- Overlapping chunks ensure context isn't lost at boundaries\n",
    "- Smaller chunks allow for more precise retrieval\n",
    "\n",
    "### Why embeddings?\n",
    "- Embeddings convert text into numerical vectors\n",
    "- Similar text has similar vectors (close in vector space)\n",
    "- Enables semantic search (finding meaning, not just keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Import the needed libraries ----------------\n",
    "from VectorStore_v2 import VectorStore\n",
    "import os\n",
    "\n",
    "# Your OpenAI API Key\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ---------------- Example usage ----------------\n",
    "kb_folder = r\"docs\"\n",
    "\n",
    "vectorstore_path = \"vectorstore\"\n",
    "vectorstore_name = \"vector_store.json\"\n",
    "\n",
    "# ‚úÖ Create folder only\n",
    "os.makedirs(vectorstore_path, exist_ok=True)\n",
    "\n",
    "# ‚úÖ Join folder + file name\n",
    "vector_store_path = os.path.join(vectorstore_path, vectorstore_name)\n",
    "\n",
    "# Initialize Vector Store\n",
    "store = VectorStore(api_key=os.getenv(\"OPENAI_API_KEY\"), chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Extract text, create and save Vector Store\n",
    "store.exract_save_vector_store(store, kb_folder, vector_store_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570f4948",
   "metadata": {},
   "source": [
    "## Step 2: Query the Knowledge Base with RAG\n",
    "\n",
    "This step demonstrates how to ask questions and get answers based on your documents.\n",
    "\n",
    "### What happens here:\n",
    "1. **Load Vector Store**: Loads the previously created vector store from JSON\n",
    "2. **Embed Query**: Converts your question into an embedding vector\n",
    "3. **Similarity Search**: Finds the top-k most relevant document chunks using cosine similarity\n",
    "4. **Retrieve Context**: Gets the actual text from the most similar chunks\n",
    "5. **Generate Answer**: Sends query + context to the LLM for a context-aware response\n",
    "\n",
    "### How similarity search works:\n",
    "- **Cosine Similarity**: Measures the angle between query and document vectors\n",
    "- **Top-k Retrieval**: Returns the k most similar chunks (here, k=3)\n",
    "- **Context Assembly**: Combines retrieved chunks to provide comprehensive context\n",
    "\n",
    "### Benefits:\n",
    "- ‚úÖ Answers are grounded in your actual documents\n",
    "- ‚úÖ Can cite specific sources (chunks used)\n",
    "- ‚úÖ Reduces hallucinations (AI making up information)\n",
    "- ‚úÖ Works with documents larger than LLM context windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee23d8d",
   "metadata": {},
   "source": [
    "## Try Your Own Questions!\n",
    "\n",
    "Modify the query below to ask questions about the ETIIAC_2025 document. The RAG system will:\n",
    "- Find the most relevant sections from the document\n",
    "- Use that context to generate an accurate answer\n",
    "- Show you which chunks were used (for transparency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b70baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Import the needed libraries ----------------\n",
    "import os\n",
    "from querykb_v2 import RAG\n",
    "import textwrap\n",
    "\n",
    "# ---------------- Example usage ----------------\n",
    "vectorstore_path = \"vectorstore\"\n",
    "vectorstore_name = \"vector_store.json\"\n",
    "\n",
    "vectorstore_full_path = os.path.join(vectorstore_path, vectorstore_name)\n",
    "\n",
    "rag = RAG(vector_store_path=vectorstore_full_path)\n",
    "\n",
    "\n",
    "system_prompt = \"You are a helpful assistant\"\n",
    "user_msg = \"What is this document about?\"\n",
    "\n",
    "answer, used_context = rag.askAI(user_msg, system_prompt, k=3)\n",
    "\n",
    "print(\"\\nContext used:\\n\", used_context)\n",
    "print(\"\\n\")\n",
    "print(f\"Reply Text:\\n {textwrap.fill(answer, width=80)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7102184a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We've Demonstrated:\n",
    "\n",
    "1. **Agentic AI**: Direct API interaction with LLMs using OpenRouter\n",
    "   - Simple, stateless conversation\n",
    "   - Access to multiple free models\n",
    "   - No document context (general knowledge only)\n",
    "\n",
    "2. **RAG System**: Enhanced AI with document knowledge\n",
    "   - Document processing and chunking\n",
    "   - Vector embeddings for semantic search\n",
    "   - Context-aware question answering\n",
    "   - Grounded in your specific documents\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Feature | Basic AI Agent | RAG System |\n",
    "|---------|---------------|------------|\n",
    "| Knowledge Source | Pre-trained model | Your documents |\n",
    "| Accuracy | General knowledge | Document-specific |\n",
    "| Hallucinations | Possible | Reduced |\n",
    "| Context Window | Limited | Can handle large docs |\n",
    "| Use Case | General Q&A | Domain-specific Q&A |\n",
    "\n",
    "### Next Steps:\n",
    "- Try different questions with the RAG system\n",
    "- Experiment with different chunk sizes and overlap\n",
    "- Adjust the `k` parameter (number of chunks retrieved)\n",
    "- Try different free LLM models for comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b010402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
